I have been engaging with AI existential safety issues on LessWrong and elsewhere quite a bit in the last few months.

Here is my current understanding.

***

The mainline view of OpenAI (and, I think, Microsoft) is as follows.

  * Short timelines and smooth gradual takeoff might be our best bet, let's aim for that (let's _"cautiously accelerate"_).

  * Copilot-style interactions are the safest and most productive, let's try to keep it this way long into the future (at least until some people start merging with AIs)

  * We'll use AI assistance for everything (helping us to align AI tools, helping us to aim AI safely, helping us to organize society in such a way that people would not want to aim AI in unsafe ways)

This talk with Ilya Sutskever, "What AI is Making Possible", https://www.youtube.com/watch?v=xym5f0XYlSc is a good illustration

  * See, in particular, his discussion at 16:04-22:06, so 6 minutes of that.

***

Because of this focus, the issues of AI existential safety for completely autonomous AI can hopefully be pushed long into the future (and to the time when there is no firm boundary between humanity and AI, due to Neuralink-style integration for willing participants and such).

***

Stepping back, the uncertainty about possible future scenarios is really high: https://www.lesswrong.com/posts/SRW9WAEEKJEgHAhSy/60-possible-futures

***

I do disagree with the dominant paradigm of "alignment of AI to some ill-defined notion of human values". In any case, the notion of alignment is not well-defined either: https://www.lesswrong.com/posts/ZKeNbGBf36ZEgDEKD/types-and-degrees-of-alignment

***

Whether we are talking about autonomous AI (AI ecosystems) or about human-AI communities, good properties defending us against X-risks and S-risks and other bad outcomes would be required, see e.g. my April 3 essay: [Exploring non-anthropocentric aspects of AI existential safety](https://www.lesswrong.com/posts/WJuASYDnhZ8hs5CnD/exploring-non-anthropocentric-aspects-of-ai-existential)

I am starting to think in terms of **"semi-alignment"** for an open-ended AI ecosystem, which would make it as benign as at all possible with respect to X-risks and S-risks, for example, by making sure it cares a lot about **"interests, freedom, and well-being of all sentient beings"**, or some modification of that, but would not constrain it otherwise with respect to its open-ended creative evolution. Progress towards achieving such properties and making them invariant with respect to future evolution and self-improvement is important both for a relatively autonomous AI ecosystem and for a tightly coupled AI-human community. This direction seems to be relatively unexplored (people are mostly focusing on "classical alignment" instead, on "steering and controlling a system more powerful than oneself"; I don't think this is a good long-term direction).

***

Human _intelligence amplification_ via "gradual and partial merging with AIs" is the key.

People seem to be mostly missing an important intermediate step between Copilot-style intelligence amplification and Neuralink-style intelligence amplification: tight coupling via non-invasive BCI (which should be safer, easier, cheaper, and faster to accomplish than Neuralink-style). Although some people are starting to think in this direction.

***

Unexpected, uncontrolled foom is a risk - what should we do about this (now and in the future)? How to make sure that any foom process does maintain some kind of semi-alignment and sufficient care about conservation and about well-being and autonomy of others?
