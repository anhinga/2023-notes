I have been engaging with AI existential safety issues on LessWrong and elsewhere quite a bit in the last few months.

So, here is my current understanding.

The mainline view of OpenAI (and, I think, Microsoft) is as follows.

  * Short timelines and smooth gradual takeoff might be our best bet, let's aim for that.
  * Copilot-style interactions are the safest and most productive, let's try to keep it this way long into the future (at least until some people start merging with AIs)
  * We'll use AI assistance for everything (helping us to align AI tools, helping us to aim AI safely, helping us to organize society in such a way that people would not want to aim AI in unsafe ways)

This talk with Ilya Sutskever, "What AI is Making Possible", https://www.youtube.com/watch?v=xym5f0XYlSc is a good illustration

  * See, in particular, his discussion at 16:04-22:06, so 6 minutes of that.

***
