# Non-anthropocentric approaches to AI existential safety

I have been looking at issues related to technological singularity and AI existential safety since 1998.

This year I have returned to this topic because of the seismic shocks associated with the latest LLM revolution.

Looking back, it is clear that I was always focusing on **non-anthropocentric approaches**. This makes my research
sufficiently unusual, and so it makes sense to continue it and to keep good notes.

Here is what I wrote in December 2012:

> **Thinking about talks at AGI-Impacts 2012 conference**
> 
> The current research and discussion on Friendly AI is very abstract. In particular, we are not really using software tools to help us understand the issues involved in Friendly AI, and we are not developing any detailed plans to do so. And this is the area where we really need computers to help us think.
>
> We also need to think more how we might arrive at some consensus regarding ethics not so much among us, but between us and the developing AGI, and we need to focus more on how we are going to develop such ethics jointly with AGI, so that it is as much its creation as it is ours.
>
> The idea of trying to control or manipulate an entity which is much smarter than a human does not seem ethical, feasible, or wise. What we might try to aim for is a respectful interaction. 

One thing which is no longer true today is "we are not really using software tools to help us understand the issues involved in Friendly AI";
in fact, all fruitful progress in the field is currently associated with using AI to help solving various relevant issues.

Speaking of 2023, I was the second author of _"Safety without alignment"_ paper and I wrote LessWrong post
_"Exploring non-anthropocentric aspects of AI existential safety"_ and a number of related comments on the site.

There have been also a number of privately shared notes. 

